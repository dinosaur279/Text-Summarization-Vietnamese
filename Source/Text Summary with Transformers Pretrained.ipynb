{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"%%capture\n# Install the vncorenlp python wrapper\n!pip install vncorenlp\n\n# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n!mkdir -p vncorenlp/models/wordsegmenter\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n!mv vi-vocab vncorenlp/models/wordsegmenter/\n!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/","metadata":{"id":"-N0WYucC4LW1","execution":{"iopub.status.busy":"2022-05-15T10:39:40.138201Z","iopub.execute_input":"2022-05-15T10:39:40.138531Z","iopub.status.idle":"2022-05-15T10:40:00.131843Z","shell.execute_reply.started":"2022-05-15T10:39:40.138444Z","shell.execute_reply":"2022-05-15T10:40:00.130757Z"}}},{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport string\nimport unicodedata\nfrom random import randint\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"id":"0QRk50sKyMST","execution":{"iopub.status.busy":"2022-05-15T10:40:00.134013Z","iopub.execute_input":"2022-05-15T10:40:00.134411Z","iopub.status.idle":"2022-05-15T10:40:01.133951Z","shell.execute_reply.started":"2022-05-15T10:40:00.134376Z","shell.execute_reply":"2022-05-15T10:40:01.133241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = r\"../input/summary-vietnamese/data_summary.csv\"\ndf = pd.read_csv(filename).drop('Unnamed: 0',1)\nprint(f'Dataset size: {len(df)}')\ndf.sample(5)","metadata":{"id":"j1ZbxZgry0TF","execution":{"iopub.status.busy":"2022-05-15T10:40:01.135204Z","iopub.execute_input":"2022-05-15T10:40:01.135445Z","iopub.status.idle":"2022-05-15T10:40:14.946891Z","shell.execute_reply.started":"2022-05-15T10:40:01.135414Z","shell.execute_reply":"2022-05-15T10:40:14.946066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Loại bỏ các giá trị trùng lặp và giá trị rỗng'''\n\ndf.drop_duplicates(subset=['Text'],inplace=True) #dropping duplicates\ndf.dropna(axis=0,inplace=True) #dropping na\ndf = df.reset_index().drop('index', axis = 1)\nprint(f'Dataset size: {len(df)}')\ndf.head(5)","metadata":{"id":"UMg1miuZnGfb","execution":{"iopub.status.busy":"2022-05-15T10:40:14.949311Z","iopub.execute_input":"2022-05-15T10:40:14.950901Z","iopub.status.idle":"2022-05-15T10:40:15.451462Z","shell.execute_reply.started":"2022-05-15T10:40:14.950867Z","shell.execute_reply":"2022-05-15T10:40:15.45069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{"id":"z69wM1PczqCR"}},{"cell_type":"code","source":"def cleanWord(s):\n    miss = ['!','@','#','$','%','^','&','*','(',')','-','–','_','+','=','{','[','}',']','|',':',';','?','/','<','>','~','`','‘','’',',','()','“','\"', '...','”', '“”','\\\\', '>>', '>>>']\n    for i in miss:\n        s = s.replace(i,'')\n    return re.sub(r'\\'', '', s)\n\ndef remove_link(string):\n    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', string)\n\ndef remove_extra_whitespace(string):\n    text = re.sub(r'\\s+', ' ', string).strip()\n    return text\n\ndef lower_word(data):\n    return data.lower()\n\n# Remove numbers from text\ndef rm_number_from_text(text):\n    text = re.sub('[0-9]+', '', text)\n    return ' '.join(text.split())  # to rm `extra` white space\n\n# Remove puncuation from word\ndef rm_punc_from_word(word):\n    clean_alphabet_list = [alphabet for alphabet in word if alphabet not in string.punctuation]\n    return ''.join(clean_alphabet_list)\n\n# Remove puncuation from text\ndef rm_punc_from_text(text):\n    clean_word_list = [rm_punc_from_word(word) for word in text]\n    return ''.join(clean_word_list)\n\n\n# Cleaning text\ndef clean_text(text):\n    text = text.lower()\n    text = rm_number_from_text(text)\n    text = rm_punc_from_text(text)\n\n    # there are hyphen(–) in many titles, so replacing it with empty str\n    # this hyphen(–) is different from normal hyphen(-)\n    text = re.sub('–', '', text)\n    text = ' '.join(text.split())  # removing `extra` white spaces\n\n    # Removing unnecessary characters from text\n    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n\n    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n\n    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n\n    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',str(text)).lower()\n\n    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    try:\n        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n        repl_url = url.group(3)\n        text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n    except Exception as e:\n        pass\n\n    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    return text\n\ndef solve(string):\n    func = [lower_word, remove_link, remove_extra_whitespace, cleanWord, clean_text]\n    for i in func:\n        string = i(string)\n    return string","metadata":{"id":"XzNLs_KHzqwT","execution":{"iopub.status.busy":"2022-05-15T10:40:15.453375Z","iopub.execute_input":"2022-05-15T10:40:15.453726Z","iopub.status.idle":"2022-05-15T10:40:15.478736Z","shell.execute_reply.started":"2022-05-15T10:40:15.453681Z","shell.execute_reply":"2022-05-15T10:40:15.477831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Text = df.Text.apply(lambda x: solve(str(x)))\ndf.Summary = df.Summary.apply(lambda x: solve(str(x)))\ndf.sample(5)","metadata":{"id":"GYvCsHFqz6Nd","execution":{"iopub.status.busy":"2022-05-15T10:40:15.480309Z","iopub.execute_input":"2022-05-15T10:40:15.480789Z","iopub.status.idle":"2022-05-15T10:48:22.810055Z","shell.execute_reply.started":"2022-05-15T10:40:15.480745Z","shell.execute_reply":"2022-05-15T10:48:22.809376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding what should be the **maximum length** of **Text** and **Summary** that will be feed or accepted by the learning algorithm","metadata":{"id":"13v_5m272iGQ"}},{"cell_type":"code","source":"text_count = [len(sentence.split()) for sentence in df.Text]\nsummary_count = [len(sentence.split()) for sentence in df.Summary]\n\npd.DataFrame({'Text': text_count, 'Summary': summary_count}).hist(bins=100, figsize=(16, 4))\nplt.show()","metadata":{"id":"GO0MTLFo2cmx","execution":{"iopub.status.busy":"2022-05-15T10:48:22.811386Z","iopub.execute_input":"2022-05-15T10:48:22.813015Z","iopub.status.idle":"2022-05-15T10:48:28.765152Z","shell.execute_reply.started":"2022-05-15T10:48:22.812972Z","shell.execute_reply":"2022-05-15T10:48:28.764474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check how many rows in a column has length (of the text) <= limit\ndef get_word_percent(column, limit):\n    count = 0\n    for sentence in column:\n        if len(sentence.split()) <= limit:\n            count += 1\n\n    return round(count / len(column), 2)\n\n\n# Check how many % of summary have 0-60 words\nprint(get_word_percent(df.Summary, 60))\n\n# Check how many % of text have 0-300 words\nprint(get_word_percent(df.Text, 400))","metadata":{"id":"pHOeZlhB2krX","execution":{"iopub.status.busy":"2022-05-15T10:48:28.76656Z","iopub.execute_input":"2022-05-15T10:48:28.766844Z","iopub.status.idle":"2022-05-15T10:48:34.45097Z","shell.execute_reply.started":"2022-05-15T10:48:28.766807Z","shell.execute_reply":"2022-05-15T10:48:34.45015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_text_len = 300\nmax_summary_len = 60","metadata":{"id":"Q2RheOhj2ngk","execution":{"iopub.status.busy":"2022-05-15T10:48:34.452154Z","iopub.execute_input":"2022-05-15T10:48:34.452914Z","iopub.status.idle":"2022-05-15T10:48:34.459102Z","shell.execute_reply.started":"2022-05-15T10:48:34.452872Z","shell.execute_reply":"2022-05-15T10:48:34.45833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select the summary and text between their defined max lens respectively\ndef trim_text_and_summary(df, max_text_len, max_summary_len):\n    cleaned_text = np.array(df['Text'])\n    cleaned_summary = np.array(df['Summary'])\n\n    short_text = []\n    short_summary = []\n\n    for i in range(len(cleaned_text)):\n        if len(cleaned_text[i].split()) <= max_text_len and len(\n            cleaned_summary[i].split()\n        ) <= max_summary_len:\n            short_text.append(cleaned_text[i])\n            short_summary.append(cleaned_summary[i])\n\n    df = pd.DataFrame({'Text': short_text, 'Summary': short_summary})\n    return df\n\n\ndf = trim_text_and_summary(df, max_text_len, max_summary_len)\nprint(f'Dataset size: {len(df)}')\ndf.sample(5)","metadata":{"id":"ecPz6e8g21yv","execution":{"iopub.status.busy":"2022-05-15T10:48:34.463314Z","iopub.execute_input":"2022-05-15T10:48:34.464147Z","iopub.status.idle":"2022-05-15T10:48:39.53706Z","shell.execute_reply.started":"2022-05-15T10:48:34.464107Z","shell.execute_reply":"2022-05-15T10:48:39.53635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') ","metadata":{"id":"TjEDsCQp3ik5","execution":{"iopub.status.busy":"2022-05-15T10:48:39.538311Z","iopub.execute_input":"2022-05-15T10:48:39.539027Z","iopub.status.idle":"2022-05-15T10:48:45.048543Z","shell.execute_reply.started":"2022-05-15T10:48:39.538988Z","shell.execute_reply":"2022-05-15T10:48:45.047695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_content(string):\n    original = rdrsegmenter.tokenize(string)\n    original = ' '.join([' '.join(x) for x in original])\n\n    return original","metadata":{"id":"QFZnFl0L4oIV","execution":{"iopub.status.busy":"2022-05-15T10:48:45.05205Z","iopub.execute_input":"2022-05-15T10:48:45.052736Z","iopub.status.idle":"2022-05-15T10:48:45.058322Z","shell.execute_reply.started":"2022-05-15T10:48:45.052694Z","shell.execute_reply":"2022-05-15T10:48:45.057591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"read_content(df.Text.iloc[1])","metadata":{"id":"pIqizGJo5Lc0","execution":{"iopub.status.busy":"2022-05-15T10:48:45.059691Z","iopub.execute_input":"2022-05-15T10:48:45.060431Z","iopub.status.idle":"2022-05-15T10:48:45.280074Z","shell.execute_reply.started":"2022-05-15T10:48:45.060332Z","shell.execute_reply":"2022-05-15T10:48:45.279392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Text = df.Text.apply(lambda x: read_content(str(x)))\ndf.Summary = df.Summary.apply(lambda x: read_content(str(x)))\ndf.sample(5)","metadata":{"id":"E_8AcQY66QKV","execution":{"iopub.status.busy":"2022-05-15T10:48:45.28397Z","iopub.execute_input":"2022-05-15T10:48:45.286248Z","iopub.status.idle":"2022-05-15T10:51:14.435427Z","shell.execute_reply.started":"2022-05-15T10:48:45.286208Z","shell.execute_reply":"2022-05-15T10:51:14.434591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.rename(columns={\"Summary\": \"summary\", \"Text\": \"original\"})","metadata":{"id":"LxA0-GhnA71I","execution":{"iopub.status.busy":"2022-05-15T10:51:14.439291Z","iopub.execute_input":"2022-05-15T10:51:14.439801Z","iopub.status.idle":"2022-05-15T10:51:14.451636Z","shell.execute_reply.started":"2022-05-15T10:51:14.439753Z","shell.execute_reply":"2022-05-15T10:51:14.450547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"sO0-uzARJoHI","execution":{"iopub.status.busy":"2022-05-15T10:51:14.456431Z","iopub.execute_input":"2022-05-15T10:51:14.456737Z","iopub.status.idle":"2022-05-15T10:51:14.483092Z","shell.execute_reply.started":"2022-05-15T10:51:14.456699Z","shell.execute_reply":"2022-05-15T10:51:14.482306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install datasets==1.0.2\n!pip install transformers\n\nimport datasets\nimport transformers","metadata":{"id":"tcgd9urMJoiO","execution":{"iopub.status.busy":"2022-05-15T10:51:14.484465Z","iopub.execute_input":"2022-05-15T10:51:14.484907Z","iopub.status.idle":"2022-05-15T10:51:43.707477Z","shell.execute_reply.started":"2022-05-15T10:51:14.484865Z","shell.execute_reply":"2022-05-15T10:51:43.706698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast,AutoTokenizer\n\n# phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n\n# For transformers v4.x+: \ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n\n# tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n\n# train_data = datasets.load_dataset(\"xsum\", split=\"train\")\n# val_data = datasets.load_dataset(\"xsum\", split=\"validation[:10%]\")","metadata":{"id":"Ex16X1P_K3Qo","execution":{"iopub.status.busy":"2022-05-15T10:51:43.709168Z","iopub.execute_input":"2022-05-15T10:51:43.709418Z","iopub.status.idle":"2022-05-15T10:51:50.235934Z","shell.execute_reply.started":"2022-05-15T10:51:43.709383Z","shell.execute_reply":"2022-05-15T10:51:50.235126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\ntrain_data, val_data = train_test_split(df, test_size=0.2)\ntrain_data =  Dataset.from_pandas(train_data)\nval_data =  Dataset.from_pandas(val_data)","metadata":{"id":"sTLrgpXIK_Zb","execution":{"iopub.status.busy":"2022-05-15T10:51:50.237358Z","iopub.execute_input":"2022-05-15T10:51:50.237599Z","iopub.status.idle":"2022-05-15T10:51:50.549305Z","shell.execute_reply.started":"2022-05-15T10:51:50.237568Z","shell.execute_reply":"2022-05-15T10:51:50.548569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=16 # change to 16 for full training\nencoder_max_length=256\ndecoder_max_length=64\n\ndef process_data_to_model_inputs(batch):                                                               \n    # Tokenizer will automatically set [BOS] <text> [EOS]                                               \n    inputs = tokenizer(batch[\"original\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n    outputs = tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n                                                                                                        \n    batch[\"input_ids\"] = inputs.input_ids                                                               \n    batch[\"attention_mask\"] = inputs.attention_mask                                                     \n    batch[\"decoder_input_ids\"] = outputs.input_ids                                                      \n    batch[\"labels\"] = outputs.input_ids.copy()                                                          \n    # mask loss for padding                                                                             \n    batch[\"labels\"] = [                                                                                 \n        [-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]\n    ]                     \n    batch[\"decoder_attention_mask\"] = outputs.attention_mask                                                                              \n                                                                                                         \n    return batch  \n\n# only use 32 training examples for notebook - DELETE LINE FOR FULL TRAINING\n# train_data = train_data.select(range(32))\n\ntrain_data_batch = train_data.map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size, \n    remove_columns=[\"original\", \"summary\"],\n)\ntrain_data_batch.set_format(\n    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n)\n\n\n# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n# val_data = val_data.select(range(16))\n\nval_data_batch = val_data.map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size, \n    remove_columns=[\"original\", \"summary\"],\n)\nval_data_batch.set_format(\n    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n)","metadata":{"id":"xIwwZVbLLCjG","execution":{"iopub.status.busy":"2022-05-15T10:51:50.550549Z","iopub.execute_input":"2022-05-15T10:51:50.550833Z","iopub.status.idle":"2022-05-15T10:52:23.579655Z","shell.execute_reply.started":"2022-05-15T10:51:50.55079Z","shell.execute_reply":"2022-05-15T10:52:23.578929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import EncoderDecoderModel\n\n# set encoder decoder tying to True\nroberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(\"vinai/phobert-base\", \"vinai/phobert-base\", tie_encoder_decoder=True)\n# roberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(\"roberta-base\", \"roberta-base\", tie_encoder_decoder=True)","metadata":{"id":"wRWMck0RO61M","execution":{"iopub.status.busy":"2022-05-15T10:52:23.581112Z","iopub.execute_input":"2022-05-15T10:52:23.58139Z","iopub.status.idle":"2022-05-15T10:52:41.762353Z","shell.execute_reply.started":"2022-05-15T10:52:23.581354Z","shell.execute_reply":"2022-05-15T10:52:41.761452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set special tokens\nroberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id                                             \nroberta_shared.config.eos_token_id = tokenizer.eos_token_id\n\n# sensible parameters for beam search\n# set decoding params                               \nroberta_shared.config.max_length = 64\nroberta_shared.config.early_stopping = True\nroberta_shared.config.no_repeat_ngram_size = 3\nroberta_shared.config.length_penalty = 2.0\nroberta_shared.config.num_beams = 4\nroberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size  ","metadata":{"id":"mCTui4B_PCXC","execution":{"iopub.status.busy":"2022-05-15T10:52:41.763945Z","iopub.execute_input":"2022-05-15T10:52:41.764185Z","iopub.status.idle":"2022-05-15T10:52:41.772013Z","shell.execute_reply.started":"2022-05-15T10:52:41.76415Z","shell.execute_reply":"2022-05-15T10:52:41.769253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!rm seq2seq_trainer.py\n!wget https://github.com/huggingface/transformers/blob/main/examples/legacy/seq2seq/seq2seq_trainer.py\n\n!pip install git-python==1.0.3\n!pip install sacrebleu==1.4.12\n!pip install rouge_score\n\nfrom transformers import Seq2SeqTrainer\nfrom transformers import TrainingArguments\nfrom dataclasses import dataclass, field\nfrom typing import Optional","metadata":{"id":"kP4ynAy8PGXS","execution":{"iopub.status.busy":"2022-05-15T10:52:41.77342Z","iopub.execute_input":"2022-05-15T10:52:41.7739Z","iopub.status.idle":"2022-05-15T10:53:18.509597Z","shell.execute_reply.started":"2022-05-15T10:52:41.773862Z","shell.execute_reply":"2022-05-15T10:53:18.508709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Seq2SeqTrainingArguments(TrainingArguments):\n    label_smoothing: Optional[float] = field(\n        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n    )\n    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n    predict_with_generate: bool = field(\n        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n    )\n    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n    encoder_layerdrop: Optional[float] = field(\n        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n    )\n    decoder_layerdrop: Optional[float] = field(\n        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n    )\n    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n    attention_dropout: Optional[float] = field(\n        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n    )\n    lr_scheduler: Optional[str] = field(\n        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n    )","metadata":{"id":"_bNs3KKzQixe","execution":{"iopub.status.busy":"2022-05-15T10:53:18.511242Z","iopub.execute_input":"2022-05-15T10:53:18.511532Z","iopub.status.idle":"2022-05-15T10:53:18.529527Z","shell.execute_reply.started":"2022-05-15T10:53:18.511494Z","shell.execute_reply":"2022-05-15T10:53:18.52867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\n# load rouge for validation\nrouge = datasets.load_metric(\"rouge\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    # all unnecessary tokens are removed\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n\n    return {\n        \"rouge2_precision\": round(rouge_output.precision, 4),\n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n    }","metadata":{"id":"jFfkeCuTQlxC","execution":{"iopub.status.busy":"2022-05-15T10:53:18.533261Z","iopub.execute_input":"2022-05-15T10:53:18.534198Z","iopub.status.idle":"2022-05-15T10:53:20.296801Z","shell.execute_reply.started":"2022-05-15T10:53:18.534154Z","shell.execute_reply":"2022-05-15T10:53:20.296055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set training arguments - these params are not really tuned, feel free to change\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir= './small-datasets-checkpoints/',\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    predict_with_generate=True,\n    # evaluate_during_training=True,\n    do_train=True,\n    do_eval=True,\n    logging_steps=200,  # set to 2000 for full training\n    save_steps=5000,  # set to 500 for full training\n    eval_steps=7500,  # set to 7500 for full training\n    warmup_steps=3000,  # set to 3000 for full training\n    num_train_epochs=5, #uncomment for full training\n    overwrite_output_dir=True,\n    save_total_limit=50,\n    fp16=True,\n)\n\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=roberta_shared,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data_batch,\n    eval_dataset=val_data_batch,\n)\ntrainer.train()","metadata":{"id":"MN_IEm9TQpwR","execution":{"iopub.status.busy":"2022-05-15T10:53:20.298009Z","iopub.execute_input":"2022-05-15T10:53:20.298795Z","iopub.status.idle":"2022-05-15T11:53:22.395202Z","shell.execute_reply.started":"2022-05-15T10:53:20.298754Z","shell.execute_reply":"2022-05-15T11:53:22.394466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir 'training'","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:14:18.418673Z","iopub.execute_input":"2022-05-15T12:14:18.41927Z","iopub.status.idle":"2022-05-15T12:14:19.228784Z","shell.execute_reply.started":"2022-05-15T12:14:18.419229Z","shell.execute_reply":"2022-05-15T12:14:19.227838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gsutil -m cp -r './training1/*' 'gs://kaggle-vbdi-test/training_Data'","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:17:42.550298Z","iopub.execute_input":"2022-05-15T12:17:42.55057Z","iopub.status.idle":"2022-05-15T12:17:45.213357Z","shell.execute_reply.started":"2022-05-15T12:17:42.550541Z","shell.execute_reply":"2022-05-15T12:17:45.212465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nfrom transformers import RobertaTokenizer, EncoderDecoderModel, AutoTokenizer\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom sklearn.model_selection import train_test_split\n\n# model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n# tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n\n# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n# model = EncoderDecoderModel.from_pretrained(\"./small-datasets-checkpoints/checkpoint-7600\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=True)\nmodel = EncoderDecoderModel.from_pretrained(\"./training1/checkpoint-4000\")\nmodel.to(\"cuda\")\n\n# test_data = datasets.load_dataset(\"xsum\", split=\"test\")\n\nbatch_size = 16  # change to 64 for full evaluation\n\n# map data correctly\ndef generate_summary(batch):\n    # Tokenizer will automatically set [BOS] <text> [EOS]\n    inputs = tokenizer(batch[\"original\"], padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n    input_ids = inputs.input_ids.to(\"cuda\")\n    attention_mask = inputs.attention_mask.to(\"cuda\")\n\n    outputs = model.generate(input_ids, attention_mask=attention_mask)\n\n    # all special tokens including will be removed\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    batch[\"pred\"] = output_str\n\n    return batch\n\nresults = val_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"original\"])\n\npred_str = results[\"pred\"]\nlabel_str = results[\"summary\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:15:27.711443Z","iopub.execute_input":"2022-05-15T12:15:27.71175Z","iopub.status.idle":"2022-05-15T12:15:31.636543Z","shell.execute_reply.started":"2022-05-15T12:15:27.711711Z","shell.execute_reply":"2022-05-15T12:15:31.635432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:13:32.574907Z","iopub.execute_input":"2022-05-15T12:13:32.575163Z","iopub.status.idle":"2022-05-15T12:13:34.977301Z","shell.execute_reply.started":"2022-05-15T12:13:32.575134Z","shell.execute_reply":"2022-05-15T12:13:34.976527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key,value in rouge_output.items():\n    print(key)\n    print(value.mid)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T12:13:34.978756Z","iopub.execute_input":"2022-05-15T12:13:34.979003Z","iopub.status.idle":"2022-05-15T12:13:34.986533Z","shell.execute_reply.started":"2022-05-15T12:13:34.978967Z","shell.execute_reply":"2022-05-15T12:13:34.985787Z"},"trusted":true},"execution_count":null,"outputs":[]}]}